{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim as gs\n",
    "import simplejson as json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "# from nltk import RegexpTokenizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(x):\n",
    "    return np.log(10/x)\n",
    "\n",
    "x = np.linspace(1,10,num=10)\n",
    "plt.plot(x,idf(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PreprocessIMDBData:\n",
    "    def __init__(self):\n",
    "        self.pattern = \"[\\w]+\"\n",
    "        self.stopword_set = set(stopwords.words('english'))\n",
    "        \n",
    "    def tokenizeSentence(self,text):\n",
    "        tokenized_sentence = [w.lower() for w in nltk.regexp_tokenize(text,self.pattern)]\n",
    "        cleanedSentence = list(set(tokenized_sentence).difference(self.stopword_set))\n",
    "        return cleanedSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VectorizeIMDBData:\n",
    "    \n",
    "    def __init__(self,tokenizerObj = PreprocessIMDBData()):\n",
    "        self.tokenizerObj = tokenizerObj \n",
    "        self.tfidf = TfidfVectorizer(tokenizer=tokenizerObj.tokenizeSentence, ngram_range=(0, 1))\n",
    "    \n",
    "    def getTFIDFMatrix(self,docs):\n",
    "        return self.tfidf.fit_transform(docs),self.tfidf.vocabulary_\n",
    "    \n",
    "    def getCleanedDocuments(self,docs, threshold = 0.6):\n",
    "        scores,vocab = self.getTFIDFMatrix(docs)\n",
    "        idf_inverse = 1/self.tfidf.idf_\n",
    "        new_vocab_keys = (idf_inverse <= threshold)\n",
    "        \n",
    "        new_docs = []\n",
    "        for doc_id,document in enumerate(docs):\n",
    "            new_doc = []\n",
    "            words = self.tokenizerObj.tokenizeSentence(document)\n",
    "            for word in words:\n",
    "                word_id  = vocab.get(word)\n",
    "                if word_id is not None and new_vocab_keys[word_id]:\n",
    "                    new_doc.append(word)\n",
    "            \n",
    "            cleanedDocument = ' '.join(new_doc)\n",
    "            new_docs.append(cleanedDocument)\n",
    "            \n",
    "            del new_doc\n",
    "        return new_docs\n",
    "    \n",
    "    def plot_words(self,threshold = [0.6]):\n",
    "        max_figs_in_row = 5\n",
    "        rows = int(np.ceil(len(threshold)/max_figs_in_row))\n",
    "        cols  = np.minimum(len(threshold),max_figs_in_row)\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize = (15, 9))\n",
    "        \n",
    "        vocab = self.tfidf.vocabulary_\n",
    "        idf_inverse = 1/self.tfidf.idf_\n",
    "        \n",
    "        for i, ax in enumerate(fig.axes):\n",
    "            frequencies = {}\n",
    "            for key,value in vocab.items():\n",
    "                if  value < len(idf_inverse) and idf_inverse[value] <= threshold[i]:\n",
    "                    frequencies[key] = idf_inverse[value]\n",
    "            \n",
    "            if len(frequencies) <= 0:\n",
    "                continue\n",
    "            \n",
    "            wc  = WordCloud(max_words=100)\n",
    "            wc.generate_from_frequencies(frequencies)\n",
    "            ax.imshow(wc, interpolation='bilinear')\n",
    "            ax.set_axis_off()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \n",
    "    @staticmethod\n",
    "    def getGenres(genresStr):\n",
    "        if pd.isnull(genresStr ):\n",
    "            return []\n",
    "        \n",
    "        genresInStrList = []\n",
    "        genreJson = json.loads(genresStr.replace(\"'\",'\"'))\n",
    "        for genreObject in genreJson:\n",
    "            genreName = genreObject['name'].lower()\n",
    "            genresInStrList.append(genreName)\n",
    "        \n",
    "        return genresInStrList\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_df_array(df,uniqueGenres = dict()):\n",
    "        tokenizer = PreprocessIMDBData()\n",
    "        movies = []\n",
    "        for index,row in df.iterrows():\n",
    "            movietags = []\n",
    "            tags = Utils.getGenres( row['genres'])\n",
    "            if len(uniqueGenres) != 0:\n",
    "                for tag in tags:\n",
    "                    if tag in uniqueGenres:\n",
    "                        movietags.append(tag)\n",
    "            else:\n",
    "                movietags = movietags + tags\n",
    "#             movietags.append(row['id'])\n",
    "#             movietags.append(row['title'])\n",
    "            movies.append(TaggedDocument(words = tokenizer.tokenizeSentence(row['overview_cleaned']),\n",
    "                                              tags = movietags))\n",
    "        return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mytokenizer = PreprocessIMDBData()\n",
    "myTfIdfGenerator = VectorizeIMDBData(mytokenizer)\n",
    "\n",
    "                                            #################  Test tf-idf and tokenizer\n",
    "# docs = [\"The sun is shining allowance\", \"The sun weather is sweet\", \"the is shining and the weather is sweet\"]\n",
    "# cleaned_docs = myTfIdfGenerator.getCleanedDocuments(docs,threshold=0.8)\n",
    "# print(cleaned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 44512 entries, 0 to 45465\n",
      "Data columns (total 4 columns):\n",
      "id          44512 non-null object\n",
      "title       44506 non-null object\n",
      "genres      44512 non-null object\n",
      "overview    44512 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "movies_actual = pd.read_csv('./the-movies-dataset/movies_metadata.csv',low_memory=False)\n",
    "cols = ['id','title','genres','overview']\n",
    "movies_df_clean = movies_actual[cols]\n",
    "movies_df_clean = movies_df_clean[pd.notnull(movies_df_clean['overview'])]\n",
    "movies_df_clean.info()\n",
    "del movies_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datasetOverview(df):\n",
    "    genre_count = 0\n",
    "    word_count = 0\n",
    "    for index,row in df.iterrows():\n",
    "        movietags = Utils.getGenres( row['genres'])\n",
    "        genre_count += len(movietags)\n",
    "        words = mytokenizer.tokenizeSentence(row['overview'])\n",
    "        word_count += len(words)\n",
    "    \n",
    "    print(\"Avg Genre :: \",(genre_count/len(df)))\n",
    "    print(\"Avg Words :: \",(word_count/len(df)))\n",
    "    return\n",
    "\n",
    "datasetOverview(movies_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myTfIdfGenerator.getTFIDFMatrix(docs=list(movies_df_clean['overview']))\n",
    "myTfIdfGenerator.plot_words(threshold=np.linspace(0.1,0.99,num=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overview_cleaned = myTfIdfGenerator.getCleanedDocuments(list(movies_df_clean['overview']), threshold = 0.6)\n",
    "movies_df_clean['overview_cleaned'] = overview_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_frequencies(movies_df_clean,cutoff = 0.00000001,columnName = 'overview',showGraph = False):\n",
    "    mytokenizer = PreprocessIMDBData()\n",
    "    genre_unkw = {}\n",
    "    for index,row in movies_df_clean.iterrows():\n",
    "        summary = set(mytokenizer.tokenizeSentence(row[columnName]))\n",
    "        if len(summary) <= 0:\n",
    "            continue\n",
    "        \n",
    "        genreList = Utils.getGenres(row['genres'])\n",
    "        if len(genreList) <= 0:\n",
    "            continue\n",
    "        \n",
    "        for genreObject in genreList:\n",
    "            uniqueWords = set()\n",
    "            if genreObject  in genre_unkw:\n",
    "                uniqueWords = genre_unkw[genreObject]\n",
    "            \n",
    "            uniqueWords |= summary\n",
    "            genre_unkw[genreObject] = uniqueWords\n",
    "            \n",
    "    uniqueWordsSet = set()    \n",
    "    uniqueWordsDict = {}\n",
    "    for genre,uniqueWords in genre_unkw.items():\n",
    "        uniqueGenreRep_Count = len(uniqueWords)\n",
    "        uniqueWordsSet |= uniqueWords\n",
    "        if uniqueGenreRep_Count <= cutoff * len(movies_df_clean):\n",
    "            continue\n",
    "        \n",
    "        uniqueWordsDict[genre] = uniqueGenreRep_Count\n",
    "        uniqueWordsDict['VocabSize'] = len(uniqueWordsSet)\n",
    "        \n",
    "    df = pd.DataFrame.from_dict(uniqueWordsDict, orient='index')\n",
    "    if showGraph:\n",
    "        df.plot(kind='bar')\n",
    "    \n",
    "#     del uniqueWordsDict\n",
    "#     del genre_unkw\n",
    "    return df,uniqueWordsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df,_ = plot_frequencies(movies_df_clean,cutoff = 0.1,showGraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds=np.linspace(0.1,0.99,num=10)\n",
    "\n",
    "plots_list = list()\n",
    "for threshold in thresholds:\n",
    "    print(\"Analyzing for threshold \",threshold)\n",
    "    overview_cleaned = myTfIdfGenerator.getCleanedDocuments(list(movies_df_clean['overview']), threshold = threshold)\n",
    "    movies_df_clean['overview_cleaned'] = overview_cleaned\n",
    "    df,_ = plot_frequencies(movies_df_clean,cutoff=0.1,columnName='overview_cleaned')\n",
    "    plots_list.append(df)\n",
    "    print(\"Done with analysis\")\n",
    "print(\"------------->Done<-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_figs_in_row = 3\n",
    "rows = int(np.ceil(len(plots_list)/max_figs_in_row))\n",
    "cols  = np.minimum(len(plots_list),max_figs_in_row)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize = (5*rows,9*cols))\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    if i >= len(plots_list):\n",
    "        continue\n",
    "#     plots_list[i].plot(kind='bar',ax = ax)\n",
    "    s = ['g' if index == 'VocabSize' else 'c' for index,value in plots_list[i][0].iteritems()]\n",
    "    plots_list[i].plot(kind='bar',ax = ax,color = s)\n",
    "    ax.set_xlabel(\"genre\")\n",
    "    ax.set_ylabel(\"num of unique words, {} threshold\".format(thresholds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overview_cleaned = myTfIdfGenerator.getCleanedDocuments(list(movies_df_clean['overview']), threshold = 0.5)\n",
    "movies_df_clean['overview_cleaned'] = overview_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_,uniqueGenres = plot_frequencies(movies_df_clean,cutoff=0.1,columnName='overview_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All genres have similar sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledIMDBData(object):\n",
    "    \n",
    "    def __init__(self,data,considerTags = dict()):\n",
    "        self.movies = data\n",
    "        self.uniqueGenres = considerTags\n",
    "    \n",
    "    @classmethod\n",
    "    def fromlist(cls,movies_data,considerTags = dict()):\n",
    "        return cls(movies_data,considerTags)\n",
    "    \n",
    "    @classmethod\n",
    "    def fromDataFrame(cls,movies_df,considerTags = dict()):\n",
    "        movies_data = Utils.convert_df_array(movies_df,considerTags)\n",
    "        return cls(movies_data,considerTags)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for movie_data in self.movies:\n",
    "            yield movie_data\n",
    "        return\n",
    "    \n",
    "    def to_array(self):\n",
    "        return self.movies\n",
    "    \n",
    "    def generate_permutations(self):\n",
    "        from random import shuffle\n",
    "        shuffle(self.movies)\n",
    "        return self\n",
    "    \n",
    "    def datasetSize(self):\n",
    "        return len(self.movies)\n",
    "    \n",
    "    def split_data(self,test_split = 0.3):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        movies_train_array, movies_test_array = train_test_split(self.movies, test_size=test_split, random_state=42)\n",
    "        movies_train = LabeledIMDBData.fromlist(movies_train_array,self.uniqueGenres)\n",
    "        movies_test = LabeledIMDBData.fromlist(movies_test_array,self.uniqueGenres)\n",
    "        return movies_train,movies_test\n",
    "    \n",
    "    def getVocabFreq(self):\n",
    "        vocab = {}\n",
    "        for movie_info in self:\n",
    "            overview = movie_info.words\n",
    "            for word in overview:\n",
    "                freq = 0\n",
    "                if word in vocab:\n",
    "                    freq = vocab[word]\n",
    "                freq = freq + 1\n",
    "                vocab[word] = freq\n",
    "            \n",
    "            tags = movie_info.tags\n",
    "            for tag in tags:\n",
    "                freq = 0\n",
    "                if tag in vocab:\n",
    "                    freq = vocab[tag]\n",
    "                freq = freq + 1\n",
    "                vocab[tag] =  freq\n",
    "        return vocab\n",
    "    \n",
    "    def slice_tags(self,tagToSearch):\n",
    "        movies = []\n",
    "        \n",
    "        for movie in self:\n",
    "            tags = movie.tags\n",
    "            for tag in tags:\n",
    "                if tag == tagToSearch:\n",
    "                    movies.append(movie)\n",
    "        return movies\n",
    "    \n",
    "    def slice_text(self,wordToSearch):\n",
    "        movies = []\n",
    "        \n",
    "        for movie in self:\n",
    "            overview = movie_info.words\n",
    "            for word in overview:\n",
    "                if word == wordToSearch:\n",
    "                    movies.append(movie)\n",
    "        return movies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_rate_for_model(test_model, train_set, test_set, uniqueGenres):\n",
    "    \"\"\"Generate precision recall values for train and test documents\"\"\"\n",
    "    \n",
    "    def predictLabels(model,data):\n",
    "        labels_pred = []\n",
    "        labels_actual = []\n",
    "        for movie in data:\n",
    "            prediction_vector = test_model.infer_vector(movie.words)\n",
    "            sims = test_model.docvecs.most_similar([prediction_vector],topn=2)\n",
    "            \n",
    "            predictions = []\n",
    "            for pred in sims:\n",
    "                predictions.append(pred[0])\n",
    "            \n",
    "            labels_pred.append(predictions)\n",
    "            labels_actual.append(movie.tags)\n",
    "        return labels_pred,labels_actual\n",
    "    \n",
    "    def accuracy(actual_labels,pred_labels):\n",
    "        tp = tn = fp = fn = 0\n",
    "        for actual_label, pred_label in zip(actual_labels,pred_labels):\n",
    "            for prediction in pred_label:\n",
    "                if prediction in actual_label:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            \n",
    "            for actual in actual_label:\n",
    "                if actual not in pred_label:\n",
    "                    fn += 1\n",
    "            \n",
    "            uniqueValues = set(actual_label+pred_label)\n",
    "            tn = len(uniqueGenres) - len(uniqueValues)\n",
    "            del uniqueValues\n",
    "        \n",
    "        accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
    "        precision = (tp)/(tp+fp)\n",
    "        recall = (tp)/(tp+fn)\n",
    "        return accuracy,precision,recall\n",
    "\n",
    "    train_label_pred,train_label_actual = predictLabels(test_model,train_set)\n",
    "#     print(\"--\\n\",train_label_actual,\"--\",train_label_pred,\"\\n\")\n",
    "    test_label_pred,test_label_actual = predictLabels(test_model,test_set)\n",
    "#     print(\"--\\n\",test_label_actual,\"--\\n\",test_label_pred,\"\\n\")\n",
    "    \n",
    "    train_accuracy,train_precision,train_recall = accuracy(train_label_actual,train_label_pred)\n",
    "    test_accuracy,test_precision,test_recall = accuracy(test_label_actual,test_label_pred)\n",
    "    \n",
    "    return train_accuracy,train_precision,train_recall,test_accuracy,test_precision,test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size  31158 \n",
      "Test data size  13354\n"
     ]
    }
   ],
   "source": [
    "movies_data = LabeledIMDBData.fromDataFrame(movies_df_clean,uniqueGenres)\n",
    "movies_train,movies_test = movies_data.split_data(test_split = 0.3)\n",
    "print(\"Train data size \",movies_train.datasetSize(),\"\\nTest data size \",movies_test.datasetSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=1, workers=1)\n",
    "model.build_vocab(movies_data.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now at epoch number  1  out of total number of epoch  200\n",
      "Now at epoch number  2  out of total number of epoch  200\n",
      "Now at epoch number  3  out of total number of epoch  200\n",
      "Now at epoch number  4  out of total number of epoch  200\n",
      "Now at epoch number  5  out of total number of epoch  200\n",
      "Now at epoch number  6  out of total number of epoch  200\n"
     ]
    }
   ],
   "source": [
    "alpha, min_alpha, passes = (0.025, 0.001, 200)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "accuracy =[[],[]]\n",
    "precision = [[],[]]\n",
    "recall = [[],[]]\n",
    "\n",
    "for epoch in range(passes):\n",
    "    print(\"Now at epoch number \",(epoch+1),\" out of total number of epoch \",passes)\n",
    "    \n",
    "    model.alpha, model.min_alpha = alpha, alpha\n",
    "    model.train(movies_train.generate_permutations(), total_examples=movies_train.datasetSize(), epochs=1)\n",
    "    train_accuracy,train_precision,train_recall,test_accuracy,test_precision,test_recall = accuracy_rate_for_model(model,movies_train,movies_test,uniqueGenres)\n",
    "    accuracy[0].append(train_accuracy)\n",
    "    accuracy[1].append(test_accuracy)\n",
    "    precision[0].append(train_precision)\n",
    "    precision[1].append(test_precision)\n",
    "    recall[0].append(train_recall)\n",
    "    recall[1].append(test_recall)\n",
    "    model.alpha -= alpha_delta\n",
    "#     print(model.corpus_count)\n",
    "    \n",
    "print(\"---------------------->Done with training<----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_array(data_array,colors = ['r','b'],labels = ['train','test'],titles=['accuracy','precision','recall']):\n",
    "    fig, axis = plt.subplots(1,len(data_array), figsize = (15, 3))\n",
    "    for x_idx in range(len(data_array)):\n",
    "        for y_idx in range(len(data_array[x_idx])):\n",
    "            axis[x_idx].plot(data_array[x_idx][y_idx],color = colors[y_idx],label = labels[y_idx])\n",
    "            axis[x_idx].set_title(titles[x_idx])\n",
    "            x_limits = len(data_array[x_idx][y_idx])+0.05\n",
    "            axis[x_idx].set_xlim([0,x_limits])\n",
    "            axis[x_idx].set_ylim([0,1.05])\n",
    "            axis[x_idx].legend(bbox_to_anchor=(0.3, 0.95), loc=1, borderaxespad=0.)\n",
    "    return\n",
    "\n",
    "plot_array([accuracy,precision,recall],titles=['accuracy','precision','recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./models/imdb.d2v')\n",
    "model.save_word2vec_format('./models/word2vecformat.nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_overviews = movies_test.to_array()[0].words\n",
    "test_overviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector = model.infer_vector(test_overviews)\n",
    "sims = model.docvecs.most_similar([new_vector],topn=3)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_id = str(862)\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%s): «%s»\\n' % (doc_id, ' '.join(movies_data.slice_tags(doc_id)[0].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(movies_data.slice_tags(doc_id)[0].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_id = ('animation')\n",
    "sims = model.docvecs.most_similar(doc_id, topn=20)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_id = ('comedy')\n",
    "sims = model.docvecs.most_similar(doc_id, topn=20)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(['wedding','opens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.docvecs.similarity('romance','animation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.docvecs.similarity('drama','family')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres = set()\n",
    "for idx,row in movies_df_clean.iterrows():\n",
    "    genres |= set(Utils.getGenres(row['genres']))\n",
    "\n",
    "genres = list(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "genreSimDict = {}\n",
    "for genreSublist in permutations(genres,2):\n",
    "    genreOne = genreSublist[0]\n",
    "    genreTwo = genreSublist[1]\n",
    "    if genreOne not in uniqueGenres or genreTwo not in uniqueGenres:\n",
    "        continue\n",
    "    try:\n",
    "        similarity_score = model.docvecs.similarity(genreOne,genreTwo)\n",
    "    except:\n",
    "        pass\n",
    "    genreSimDict[genreSublist] = similarity_score\n",
    "#     if similarity_score< 0.3:\n",
    "#         continue\n",
    "#     print(\"Similarity between %s,%s is %f\\n\" % (genreOne,genreTwo,similarity_score))\n",
    "ser = pd.Series(list(genreSimDict.values()),\n",
    "                  index=pd.MultiIndex.from_tuples(genreSimDict.keys()))\n",
    "df = ser.unstack().fillna(0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from gensim import corpora\n",
    "# from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# sentenceTokenizer = PreprocessIMDBData()\n",
    "# movies = movies_df_clean['overview_cleaned'].values\n",
    "# movie_tokenized = [sentenceTokenizer.tokenizeSentence(movie) for movie in movies]\n",
    "# dictionary = corpora.Dictionary(movie_tokenized)\n",
    "# doc_term_matrix = [dictionary.doc2bow(movie) for movie in movie_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ldamodel = LdaModel(doc_term_matrix,num_topics=10,id2word=dictionary,passes= 20,alpha=0.3,eta=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(ldamodel.print_topics(num_topics=10,num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
